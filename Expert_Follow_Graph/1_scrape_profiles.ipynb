{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Think Tank Experts\n",
    "* expert names\n",
    "* expert links\n",
    "* titles and short descriptions\n",
    "* twitter acounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, re, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### American Enterprise Institute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, \"html.parser\").body\n",
    "    \n",
    "    try: accounts = soup.findAll('a', {'class':'twitter-handle'})[0].text\n",
    "    except IndexError:  accounts = ''\n",
    "    short_desc = soup.findAll('h5', {'class':None})[0].text\n",
    "    scp = soup.findAll('p', {'class': None})\n",
    "    des = '\\n'.join([i.text for i in scp])\n",
    "    tag = '\\n'.join(find_tags(soup))\n",
    "    name = soup.findAll('h1')[0].text.strip('\\n').strip()\n",
    "    \n",
    "    return name, accounts, short_desc, des, tag\n",
    "\n",
    "def find_tags(sp):\n",
    "    collect = []\n",
    "    for i in sp.findAll('a'):\n",
    "        try: href = i['href']\n",
    "        except KeyError: continue\n",
    "        if 'tags_str' in href: collect.append(i.text)\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.aei.org/our-scholars/'\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.text, \"html.parser\").body\n",
    "staff_urls = [i['href'] for i in soup.findAll('a', {'class':'news-thumbnail'})]\n",
    "results = [get_info(url) for url in staff_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.DataFrame({'name': [i[0] for i in results],\n",
    "                          'twitter': [i[1] for i in results],\n",
    "                          'short_desc': [i[2] for i in results],\n",
    "                          'desc': [i[3] for i in results],\n",
    "                          'tag': [i[4] for i in results], \n",
    "                          'href': staff_urls})\n",
    "user_data['name'] = user_data['name'].apply(lambda x: x.strip())\n",
    "user_data['twitter'] = user_data['twitter'].apply(lambda x: x.strip().strip('@').strip())\n",
    "user_data = user_data.drop_duplicates()\n",
    "user_data.to_csv('draft_data/aei_twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(sp):\n",
    "    lst = sp.findAll(['h2','h3', 'a'], {'class':['title','name', 'twitter']})\n",
    "    name_info = dict()\n",
    "    for i in lst:\n",
    "        if i['class'][0] == 'name': \n",
    "            name = i.text\n",
    "            name_info[name = dict()\n",
    "            name_info[name]['href'] = i.find('a')['href']\n",
    "            name_info[name]['title'] = []\n",
    "            continue\n",
    "        if i['class'][0] == 'title':\n",
    "            name_info[name]['title'].append(i.text)\n",
    "            continue\n",
    "        if i['class'][0] == 'twitter':\n",
    "            name_info[name]['twitter'] = i.text.split('\\n')[1].strip('@')\n",
    "        else: name_info[name]['twitter'] = ''\n",
    "            \n",
    "    return name_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\r"
     ]
    }
   ],
   "source": [
    "total_data = dict()\n",
    "i = 0\n",
    "while True:\n",
    "    url = f'https://www.brookings.edu/experts/page/{i}?'\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, \"html.parser\").body\n",
    "    if len(soup.findAll('h3')) == 0: break\n",
    "    add = extract_info(soup)\n",
    "    total_data.update(add)\n",
    "    i += 1; print(i, end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = list(total_data.keys())\n",
    "twitter = [total_data[i]['twitter'] if 'twitter' in total_data[i] else '' for i in total_data]\n",
    "title = [';'.join(total_data[i]['title']) for i in total_data]\n",
    "user_data = pd.DataFrame({'name':name, 'twitter':twitter, 'title':title, \n",
    "                          'href': [total_data[i]['href'] for i in total_data]})\n",
    "user_data['name'] = user_data['name'].apply(lambda x: x.strip())\n",
    "user_data['twitter'] = user_data['twitter'].apply(lambda x: x.strip())\n",
    "user_data = user_data.drop_duplicates()\n",
    "user_data.to_csv('draft_data/brookings_twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heritage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_add(sp):\n",
    "    raw = sp.findAll(['div', 'a'], \n",
    "                   {\"class\": ['person-list-small__name', \n",
    "                              'person-list-small__twitter-handle']})\n",
    "    desc_raw = sp.findAll('p', {\"class\": ['person-list-small__title']})\n",
    "    \n",
    "    names = []\n",
    "    dat = [i.text for i in raw]\n",
    "    \n",
    "    while(len(dat)!=0):\n",
    "        if (len(dat) == 1) or (dat[1][0] != '@' and len(dat[1].split(' '))>1):\n",
    "            names.append((dat[0], None))\n",
    "            dat.remove(dat[0])\n",
    "        else:\n",
    "            names.append((dat[0], dat[1]))\n",
    "            dat.remove(dat[0])\n",
    "            dat.remove(dat[0])\n",
    "            \n",
    "    href = [i.find('a')['href'] for i in soup.findAll('div', \n",
    "                                {'class': 'person-list-small__name'})]\n",
    "            \n",
    "    add = pd.DataFrame({'name': [i[0] for i in names],\n",
    "                        'twitter': [i[1] for i in names],\n",
    "                        'title': [i.text for i in desc_raw], \n",
    "                        'href': href})\n",
    "    return add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z\r"
     ]
    }
   ],
   "source": [
    "types = []\n",
    "user_data = pd.DataFrame()\n",
    "\n",
    "for ch in range(ord('A'), ord('Z') + 1):\n",
    "    print(chr(ch), end = '\\r')\n",
    "    url = f'https://www.heritage.org/about-heritage/staff/leadership/{chr(ch)}'\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, \"html.parser\").body\n",
    "    add = gen_add(soup)\n",
    "    user_data = user_data.append(add)\n",
    "    types.extend(['leadership']*len(add))\n",
    "    \n",
    "for ch in range(ord('A'), ord('Z') + 1):\n",
    "    print(chr(ch), end = '\\r')\n",
    "    url = f'https://www.heritage.org/about-heritage/staff/experts/{chr(ch)}'\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, \"html.parser\").body\n",
    "    add = gen_add(soup)\n",
    "    user_data = user_data.append(add)\n",
    "    types.extend(['experts']*len(add))\n",
    "\n",
    "for ch in range(ord('A'), ord('Z') + 1):\n",
    "    print(chr(ch), end = '\\r')\n",
    "    url = f'https://www.heritage.org/about-heritage/staff/other/{chr(ch)}'\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, \"html.parser\").body\n",
    "    add = gen_add(soup)\n",
    "    user_data = user_data.append(add)\n",
    "    types.extend(['other']*len(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['types'] = types\n",
    "user_data['name'] = user_data['name'].apply(lambda x: x.strip())\n",
    "user_data['twitter'] = user_data['twitter'].apply(lambda x: x.strip('@').strip() if x else x)\n",
    "user_data = user_data.drop_duplicates()\n",
    "user_data.to_csv('draft_data/heritage_twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center for American Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.americanprogress.org/about/staff/'\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.text, \"html.parser\").body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaders = soup.findAll('ul', {'class': 'stafflist'})[0].findAll('li')\n",
    "name = [i.find('a').text for i in leaders]\n",
    "href = [i.find('a')['href'] for i in leaders]\n",
    "title = [i.text[len(i.find('a').text)+2:] for i in leaders]\n",
    "types = [\"leadership\"]*len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_lst = [i.text for i in soup.findAll('h2')]\n",
    "for i, ty in enumerate(types_lst):\n",
    "    target = soup.findAll('ul', {'class': 'stafflist'})[i].findAll('li')\n",
    "    name.extend([i.find('a').text for i in target])\n",
    "    href.extend([i.find('a')['href'] for i in target])\n",
    "    title.extend([i.text[len(i.find('a').text)+2:] for i in target])\n",
    "    types.extend([ty]*len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.DataFrame({'name':name, 'title':title, \n",
    "                          'type': types, 'href': href})\n",
    "user_data['name'] = user_data['name'].apply(lambda x: x.strip())\n",
    "user_data['href'] = user_data['href'].apply(lambda x: \n",
    "                    f\"https://www.americanprogress.org/person/{x.split('/')[-3]}/\")\n",
    "user_data = user_data.drop_duplicates()\n",
    "user_data.to_csv('draft_data/cap_twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aei = pd.read_csv('draft_data/aei_twitter.csv')\n",
    "brookings = pd.read_csv('draft_data/brookings_twitter.csv')\n",
    "heritage = pd.read_csv('draft_data/heritage_twitter.csv')\n",
    "cap = pd.read_csv('draft_data/cap_twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aei['name'] = aei['name'].apply(lambda x: x.split('@')[0].strip())\n",
    "aei['expert_id'] = aei['href'].apply(lambda x: x.split('/')[-2])\n",
    "aei.to_csv('draft_data/aei_twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aei = aei[['expert_id', 'name', 'twitter', 'short_desc', 'href']]\n",
    "aei = aei.rename(columns = {'short_desc': 'title'})\n",
    "heritage = heritage.rename(columns = {'types': 'type'})\n",
    "brookings['expert_id'] = brookings['href'].apply(lambda x: x.split('/')[-2])\n",
    "heritage['expert_id'] = heritage['href'].apply(lambda x: x.split('/')[-1])\n",
    "cap['expert_id'] = cap['href'].apply(lambda x: x.split('/')[-2])\n",
    "\n",
    "aei['institution'] = 'aei'\n",
    "brookings['institution'] = 'brookings'\n",
    "heritage['institution'] = 'heritage'\n",
    "cap['institution'] = 'cap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aei.append(brookings)\n",
    "data = data.append(heritage)\n",
    "data = data.append(cap)\n",
    "data = data[['expert_id', 'name', 'twitter', 'institution', 'type', 'title', 'href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '/Users/chenliang/Desktop/twitter_expert_sna/notebooks/'\n",
    "pre_data = pd.read_csv(address+'data_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "pre_data['name'] = pre_data['name'].apply(lambda x: x.split('@')[0].strip())\n",
    "dct = dict(zip(pre_data['name'], pre_data['twitter']))\n",
    "print(len(set(data['twitter'])))\n",
    "data['twitter'] = data[['name', 'twitter']].apply(lambda x: dct[x[0]] \n",
    "                  if x[0] in dct else x[1], axis=1)\n",
    "print(len(set(data['twitter'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('expert_twitters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
