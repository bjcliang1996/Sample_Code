{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywren, json, requests, time, bs4\n",
    "import pandas as pd\n",
    "from re import findall\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('expert_twitters_complete.csv', encoding = \"ISO-8859-1\")\n",
    "data = data[['expert_id', 'name', 'institution', 'href']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Enterprise Institue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_work(url):\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    raw = soup.find('div', {'class': 'search-facet periodical-name'})\n",
    "    if not raw: return None, None\n",
    "    else: raw = raw.findAll('li')\n",
    "    media_appearance = [(i['data-facet-count'], i['data-facet-query'][15:]) for i in raw]\n",
    "    \n",
    "    works = []\n",
    "    i = 1\n",
    "    while True:\n",
    "        if i == 1: new_url = url\n",
    "        else: new_url = url + f'&wpsolr_page={i}'\n",
    "        #print(new_url)\n",
    "        r = requests.get(new_url)\n",
    "        soup = bs4.BeautifulSoup(r.text)\n",
    "        raw = soup.findAll('a', {'class': 'news-thumbnail'})\n",
    "        time = [i.text for i in soup.findAll('span', {'class': 'primary-18'})]\n",
    "        if len(time) == 0: break\n",
    "        i += 1\n",
    "        works.extend(list(zip(time, [i['href'] for i in raw])))\n",
    "        #print(len(works))\n",
    "        \n",
    "    return media_appearance, works\n",
    "\n",
    "def scrape_expert_aei(user_id):\n",
    "    expert = dict()\n",
    "    href = f'https://www.aei.org/profile/{user_id}/'\n",
    "    r = requests.get(href)\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    \n",
    "    try: expert['title'] = soup.find('h5').text\n",
    "    except: return {user_id: None}\n",
    "    \n",
    "    raw = soup.find('div', {'class': 'minimize js-minimize wysiwyg'})\n",
    "    if not raw: texts = []\n",
    "    else: texts = [i.text for i in raw.findAll(['p', 'h2', 'li'])]\n",
    "    \n",
    "    try: \n",
    "        expert['bio'] = '\\n'.join(texts[:texts.index('Experience')]).strip()\n",
    "        expert['experience'] = '\\n'.join(texts[texts.index('Experience')+1: \n",
    "                             texts.index('Education')]).strip()\n",
    "        expert['education'] = '\\n'.join(texts[texts.index('Education')+1:]).strip()\n",
    "    except ValueError:\n",
    "        expert['bio'] = '\\n'.join(texts).strip()\n",
    "        expert['experience'] = ''\n",
    "        expert['education'] = ''\n",
    "    \n",
    "    work_link = [i['href'] for i in soup.findAll('a', {'class': 'cta'}) \n",
    "                 if \"View all\" in i.text]\n",
    "    assert('search-results' in work_link[0]); assert('type:event'!= work_link[0])\n",
    "    media_appearance, works = find_all_work(work_link[0])\n",
    "    expert['media_appearance'] = media_appearance\n",
    "    expert['works'] = works\n",
    "    \n",
    "    return {user_id: expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roger-bate 1013\n",
      "125\r"
     ]
    }
   ],
   "source": [
    "expert_data = dict()\n",
    "tar_accounts = list(set(data[data['institution']=='aei']['expert_id']))\n",
    "for num, account in enumerate(tar_accounts):\n",
    "    print(num, end = '\\r')\n",
    "    add = scrape_expert_aei(account)\n",
    "    if account == 'roger-bate': \n",
    "        print(account, len(add['roger-bate']['works']))\n",
    "    try: expert_data.update(add)\n",
    "    except (IndexError, AttributeError): continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('draft_data/expert_aei_v2.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_users_brookings(user_id):\n",
    "    r = requests.get(f'https://www.brookings.edu/experts/{user_id}/')\n",
    "    expert = dict()\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    try: expert['title'] = soup.find('h3', {'class': 'title'}).text\n",
    "    except AttributeError: expert['title'] = None\n",
    "    try: expert['bio'] = soup.find('div', \n",
    "                              {'class': 'expert-intro-text post-body'}).text.strip()\n",
    "    except AttributeError: expert['bio'] = None\n",
    "    try: alt_id = soup.find('div', {'class':\n",
    "                                    'section-header'}).find('a')['href'].split('/')[-2]\n",
    "    except AttributeError: alt_id = user_id\n",
    "    raw = soup.findAll(['dt', 'dd'])\n",
    "    info, key = dict(), ''\n",
    "    for i in raw:\n",
    "        if i.name == 'dt':\n",
    "            key = i.text.strip(); info[key] = []\n",
    "        else: info[key].append(i.text.strip())\n",
    "    expert.update(info)\n",
    "    expert['articles'] = find_articles(user_id, alt_id)\n",
    "    return {user_id: expert}\n",
    "\n",
    "def find_articles(user_id, alt_id):\n",
    "    articles = []\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        url = f'https://www.brookings.edu/author/{user_id}/?type=all&paged={i}'\n",
    "        print(url)\n",
    "        r = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(r.text)\n",
    "        if 'Page not found' in soup.findAll('title')[0].text: \n",
    "            if i == 0: \n",
    "                user_id = alt_id\n",
    "                url = f'https://www.brookings.edu/author/{alt_id}/?type=all&paged={i}'\n",
    "                print(url)\n",
    "                r = requests.get(url)\n",
    "                soup = bs4.BeautifulSoup(r.text)\n",
    "                if 'Page not found' in soup.findAll('title')[0].text: \n",
    "                    return []\n",
    "            else: return articles\n",
    "        raw = list(zip(soup.findAll('h4', {'class': 'title'}), soup.findAll('time')))\n",
    "        new_added = [(i[0].find('a')['href'], i[1].text.strip()) \n",
    "                    for i in raw if i[0].find('a')]\n",
    "        if len(new_added) == 0: break\n",
    "        articles.extend(new_added)\n",
    "        i += 1\n",
    "        print(len(articles))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_accounts = list(set(data[data['institution']=='brookings']['expert_id']))\n",
    "#tar_accounts = list(set(tar_accounts)-expert_data.keys())\n",
    "len(tar_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_users_brookings, tar_accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 0    \r"
     ]
    }
   ],
   "source": [
    "not_dones = [1]\n",
    "while len(not_dones) != 0: \n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    print(len(dones), len(not_dones), end = '  \\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_data = dict()\n",
    "for num, item in enumerate(dones):\n",
    "    try: expert_data.update(item.result())\n",
    "    except (IndexError, AttributeError, NameError): continue\n",
    "list(set(tar_accounts)-expert_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('draft_data/expert_brookings_v2.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heritage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.heritage.org/'\n",
    "\n",
    "def scrape_expert_heritage(expert_id):\n",
    "    expert = dict()\n",
    "    r = requests.get(f'https://www.heritage.org/staff/{expert_id}')\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    raw = soup.find('h2', {'class':'expert-bio-card__expert-title'})\n",
    "    if raw: expert['title'] = raw.text.strip()\n",
    "    else: expert['title'] = ''\n",
    "    raw = soup.find('div', {'class': 'expert-bio__read-more-container'})\n",
    "    if raw: expert['bio'] = raw.text.strip()\n",
    "    else: expert['bio'] = ''\n",
    "\n",
    "    articles = []\n",
    "    i = 0 \n",
    "    while True:\n",
    "        url = f'https://www.heritage.org/staff/{expert_id}?page={i}'\n",
    "        r = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(r.text)\n",
    "        hrefs = [base+i['href'] for i in soup.findAll('a', {'class': 'result-card__title'})]\n",
    "        time = [i.text[:-10].strip() for i in soup.findAll('p', {'class': 'result-card__date'})]\n",
    "        new_added = list(zip(hrefs, time))\n",
    "        if len(new_added) == 0: break\n",
    "        articles.extend(new_added)\n",
    "        print(i, end = '\\r')\n",
    "        i += 1\n",
    "    expert['articles'] = articles\n",
    "    \n",
    "    return {expert_id: expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "tar_accounts = list(set(data[data['institution']=='heritage']['expert_id']))\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_expert_heritage, tar_accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dones = [1]\n",
    "while len(not_dones) != 0: \n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    print(len(dones), len(not_dones), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = dict()\n",
    "for num, item in enumerate(dones):\n",
    "    expert_data.update(item.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('draft_data/expert_heritage_v2.json'， 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_expert_cap(expert_id):\n",
    "    r = requests.get(f'https://www.americanprogress.org/person/{expert_id}/')\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    \n",
    "    expert = dict()\n",
    "    try: expert['title'] = soup.find('div', {'class': 'col-md-12'}).text.strip()\n",
    "    except AttributeError: expert['title'] = ''\n",
    "    try: expert['bio'] = soup.find('div', {'class': 'bio-text'}).text.strip()\n",
    "    except AttributeError: expert['bio'] = ''\n",
    "    \n",
    "    raw = soup.find('table', {'class': 'display responsive'})\n",
    "    if raw: raw = raw.findAll('td')\n",
    "    else: \n",
    "        expert['articles'] = []\n",
    "        return {expert_id: expert}\n",
    "    raw = [i.find('a')['href'] if i.find('a') else i.text for i in raw]\n",
    "    articles = np.reshape(raw, (-1,3)).tolist()\n",
    "    expert['articles'] = articles\n",
    "    return {expert_id: expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    }
   ],
   "source": [
    "tar_accounts = list(set(data[data['institution']=='cap']['expert_id']))\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_expert_cap, tar_accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 048\r"
     ]
    }
   ],
   "source": [
    "not_dones = [1]\n",
    "while len(not_dones) != 0: \n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    print(len(dones), len(not_dones), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = dict()\n",
    "for num, item in enumerate(dones):\n",
    "    try: expert_data.update(item.result())\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('draft_data/expert_cap_v2.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merge and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('expert_info_v1.csv')\n",
    "data = pd.read_csv('expert_twitters_complete.csv', encoding = \"ISO-8859-1\")\n",
    "data = data[['expert_id', 'name', 'institution', 'href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "brookings = json.load(open('draft_data/expert_brookings_v2.json'))\n",
    "heritage = json.load(open('draft_data/expert_heritage_v2.json'))\n",
    "aei = json.load(open('draft_data/expert_aei_v2.json'))\n",
    "cap = json.load(open('draft_data/expert_cap_v2.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dict = dict()\n",
    "for dct in [brookings, heritage, aei, cap]:\n",
    "    for expert in dct:\n",
    "        expert_dict[expert] = dict()\n",
    "        expert_dict[expert]['title'] = dct[expert]['title']\n",
    "\n",
    "for expert in cap:\n",
    "    expert_dict[expert]['education'] = cap[expert]['bio']\n",
    "    expert_dict[expert]['experience'] = cap[expert]['bio']\n",
    "    expert_dict[expert]['articles'] = cap[expert]['articles']\n",
    "\n",
    "for expert in aei:\n",
    "    expert_dict[expert]['education'] = aei[expert]['education']\n",
    "    expert_dict[expert]['experience'] = aei[expert]['experience']\n",
    "    if aei[expert]['works']:\n",
    "        expert_dict[expert]['articles'] = [(i[1], i[0]) for i in aei[expert]['works']]\n",
    "    else:  expert_dict[expert]['articles'] = []\n",
    "    \n",
    "for expert in brookings:\n",
    "    if 'Education' in brookings[expert]:\n",
    "        expert_dict[expert]['education'] = brookings[expert]['Education']\n",
    "    else: expert_dict[expert]['education'] = ''\n",
    "    experiences = []\n",
    "    if 'Current Positions' in brookings[expert]: experiences.extend(brookings[expert]['Current Positions'])\n",
    "    if 'Past Positions' in brookings[expert]: experiences.extend(brookings[expert]['Past Positions'])\n",
    "    if brookings[expert]['bio']: experiences.append(brookings[expert]['bio'])\n",
    "    expert_dict[expert]['experience'] = experiences\n",
    "    expert_dict[expert]['articles'] = brookings[expert]['articles']\n",
    "    \n",
    "for expert in heritage:\n",
    "    expert_dict[expert]['education'] = heritage[expert]['bio']\n",
    "    expert_dict[expert]['experience'] = heritage[expert]['bio']\n",
    "    expert_dict[expert]['articles'] = heritage[expert]['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "brookings = json.load(open('draft_data/expert_brookings.json'))\n",
    "heritage = json.load(open('draft_data/expert_heritage.json'))\n",
    "aei = json.load(open('draft_data/expert_aei.json'))\n",
    "cap = json.load(open('draft_data/expert_cap.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for expert in cap:\n",
    "    if expert in cap:\n",
    "        expert_dict[expert]['articles'].extend(cap[expert]['articles'])\n",
    "\n",
    "for expert in aei:\n",
    "    if aei[expert]['works']:\n",
    "        expert_dict[expert]['articles'].extend([(i[1], i[0]) for i in aei[expert]['works']])\n",
    "    \n",
    "for expert in brookings:\n",
    "    expert_dict[expert]['articles'].extend(brookings[expert]['articles'])\n",
    "    \n",
    "for expert in heritage:\n",
    "    expert_dict[expert]['articles'].extend(heritage[expert]['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for expert in expert_dict:\n",
    "    expert_dict[expert]['articles'] = list(set([i[0] for i \n",
    "                                                in expert_dict[expert]['articles']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_dict, open('expert_all.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Excel for Bio Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywren, json, requests, time, bs4\n",
    "import pandas as pd\n",
    "from re import findall\n",
    "import numpy as np\n",
    "expert_dict = json.load(open('expert_all.json'))\n",
    "data = pd.read_csv('expert_twitters_complete.csv', encoding = \"ISO-8859-1\")\n",
    "data = data[['expert_id', 'name', 'institution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data['expert_id'].apply(lambda x: expert_dict[x]['title'] if x \n",
    "                                        in expert_dict else '')\n",
    "data['education'] = data['expert_id'].apply(lambda x: expert_dict[x]['education'] if \n",
    "                                            x in expert_dict else '')\n",
    "data['experience'] = data['expert_id'].apply(lambda x: expert_dict[x]['experience'] if \n",
    "                                             x in expert_dict else '')\n",
    "data.to_csv('expert_bio_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75737"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = list(set([href for expert in expert_dict for href in \n",
    "                     expert_dict[expert]['articles']]))\n",
    "article_data = pd.DataFrame({'href': list(articles)})\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from requests.exceptions import SSLError, MissingSchema, ConnectionError\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(href):\n",
    "    try: r = requests.get(href, timeout = 10)\n",
    "    except: return {href: '-1'}\n",
    "    if not r: return {href: '-1'}\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    raw = '\\n'.join([i.text for i in soup.findAll('p')])\n",
    "    raw = '\\n'.join([i.strip() for i in raw.split('\\n') if len(i.split(' ')) > 15])\n",
    "    return {href: raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dct = json.load(open('article_dct_full.json'))\n",
    "tar_articles = list(set(articles)-article_dct.keys()-{''})\n",
    "len(tar_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\r"
     ]
    }
   ],
   "source": [
    "#article_dct = dict()\n",
    "folds = 3000\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "\n",
    "for i in range(int(len(tar_articles)/folds)):\n",
    "    #if i*folds < len(article_dct): continue\n",
    "    print(i*folds, end = '\\r')\n",
    "    if i*folds+folds < len(articles):\n",
    "        tar_hrefs = tar_articles[i*folds: i*folds+folds]\n",
    "    else: tar_hrefs = tar_articles[i*folds:]\n",
    "        \n",
    "    futures = pwex.map(scrape_article, tar_hrefs)\n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    while len(not_dones) > 5:\n",
    "        dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    \n",
    "    for item in dones:\n",
    "        try: article_dct.update(item.result())\n",
    "        except (MissingSchema, SSLError, AttributeError, ConnectionError, \n",
    "                IndexError): continue\n",
    "    json.dump(article_dct, open('article_dct_full.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\r"
     ]
    }
   ],
   "source": [
    "#tar_articles = set(tar_articles) - article_dct.keys()\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_article, tar_articles)\n",
    "len(futures)\n",
    "dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "print(len(dones), len(not_dones), end  = '\\r')\n",
    "while len(not_dones) !=0:\n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    print(len(dones), len(not_dones), end  = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dones:\n",
    "    try: article_dct.update(item.result())\n",
    "    except: continue\n",
    "json.dump(article_dct, open('article_dct_full.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
