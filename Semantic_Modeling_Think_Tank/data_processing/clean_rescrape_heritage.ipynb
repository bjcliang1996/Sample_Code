{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import requests \n",
    "import bs4 \n",
    "import nltk.data\n",
    "from datetime import datetime\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('raw_data/heritage_full_text.csv')\n",
    "data['text'] = data['text'].apply(lambda x: x.strip() if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_target(x):\n",
    "    target = 0\n",
    "    if x[1] != target: return x[1]\n",
    "    url = x[0]\n",
    "    r = requests.get(url)\n",
    "    sp = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "    txt = [j for i in sp.body.findAll('div', {'class':None}) for j in \n",
    "           i.text.strip().split('\\n') if j != '']\n",
    "    try: txt = txt[(txt.index('Trade')+1):txt.index('More on This Issue')]\n",
    "    except ValueError: pass\n",
    "    return '\\n'.join(txt)\n",
    "\n",
    "data['text_cleaned'] = data[['urls', 'text']].apply(check_target, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_target(x):\n",
    "    if x[2] > 5: return x[1]\n",
    "    url = x[0]\n",
    "    r = requests.get(url)\n",
    "    sp = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    txt = [i.text for i in sp.body.findAll('p')]\n",
    "    txt_cleaned = []\n",
    "    for i in txt:\n",
    "        if (\"This piece originally appeared\" in i) or ('min read') in i: break\n",
    "        if i == 'Economy': txt_cleaned = []\n",
    "        else: txt_cleaned.append(i)\n",
    "\n",
    "    return '\\n'.join(txt)\n",
    "\n",
    "data['sent_len'] = data['text_cleaned'].apply(lambda x: \n",
    "                                              len(x.split('\\n')) if isinstance(x, str) else 0)\n",
    "\n",
    "data[\"text_cleaned\"] = data[['urls', 'text_cleaned', \n",
    "                             'sent_len']].apply(check_target, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(['text_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16655, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.to_csv('heritage_temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Based Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sent'] = data['text_cleaned'].apply(lambda x: \n",
    "                                          [j for i in x.split('\\n') \n",
    "                                           for j in tokenizer.tokenize(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = data['Sent'].sum()\n",
    "count = Counter(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [(i, count[i]) for i in count if count[i]>20]\n",
    "drop_sent = [i[0] for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_sent'] = data['Sent'].apply(lambda x: [i for i in x if i not in drop_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(lst):\n",
    "    new = []\n",
    "    for i in lst:\n",
    "        add = i.replace(u'\\xa0',' ')\n",
    "        add = i.replace(u'\\\\xa0',' ')\n",
    "        if 'http' in add: continue\n",
    "        if len(i) < 3: continue\n",
    "        if ('Analyst' in i) and ('The Heritage Foundation' in i): continue \n",
    "        if ('Authors:' in i) or ('Author:' in i): continue \n",
    "        if ('Scholar' in i) and ('The Heritage Foundation' in i): continue \n",
    "        if '.pdf' in add: continue \n",
    "        if i[0].isnumeric() and (not i[2:4].isalpha()): continue\n",
    "        if (\"[\" in i) and (\"]\" in i):\n",
    "            target = i[i.index('['): (i.index(']')+1)]\n",
    "            add = i.replace(target, '')\n",
    "        new.append(add)\n",
    "    return '\\n'.join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text_sent'] = data['cleaned_sent'].apply(combine_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data[['urls', 'author', 'date', 'category', 'cleaned_text_sent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(s):\n",
    "    if len(s.strip()) < 3: return \"\"\n",
    "    s = s.replace('th', '')\n",
    "    s = s.replace('st', '')\n",
    "    s = s.replace('rd', '')\n",
    "    s = s.replace('nd', '')\n",
    "    s = s.replace('Augu', 'Aug')\n",
    "    try: d = datetime.strptime(s, '%B %d, %Y')\n",
    "    except ValueError:\n",
    "        d = datetime.strptime(s, '%b %d, %Y')\n",
    "    return d.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_text_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.heritage.org/energy-economics/comm...</td>\n",
       "      <td>['Katie  Tubb']</td>\n",
       "      <td>2014-05-14</td>\n",
       "      <td>energy-economics</td>\n",
       "      <td>Yesterday the Senate decided to take up legisl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.heritage.org/housing/commentary/no...</td>\n",
       "      <td>['Norbert J. Michel, Ph.D.']</td>\n",
       "      <td>2018-09-13</td>\n",
       "      <td>housing</td>\n",
       "      <td>The 2008 financial crisis was a major missed o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.heritage.org/budget-and-spending/c...</td>\n",
       "      <td>['Stephen  Moore']</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>budget-and-spending</td>\n",
       "      <td>It hasn't gotten much attention, but two big b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.heritage.org/trade/report/trade-ad...</td>\n",
       "      <td>['James Sherk', 'David Muhlhausen']</td>\n",
       "      <td>2011-05-16</td>\n",
       "      <td>trade</td>\n",
       "      <td>The Obama Administration and Congress recently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.heritage.org/education/report/prom...</td>\n",
       "      <td>['Kirk Johnson']</td>\n",
       "      <td>2005-04-06</td>\n",
       "      <td>education</td>\n",
       "      <td>On April 5, 2005, the Georgetown Public Policy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  \\\n",
       "0  https://www.heritage.org/energy-economics/comm...   \n",
       "1  https://www.heritage.org/housing/commentary/no...   \n",
       "2  https://www.heritage.org/budget-and-spending/c...   \n",
       "3  https://www.heritage.org/trade/report/trade-ad...   \n",
       "4  https://www.heritage.org/education/report/prom...   \n",
       "\n",
       "                                author        date             category  \\\n",
       "0                      ['Katie  Tubb']  2014-05-14     energy-economics   \n",
       "1         ['Norbert J. Michel, Ph.D.']  2018-09-13              housing   \n",
       "2                   ['Stephen  Moore']  2015-10-19  budget-and-spending   \n",
       "3  ['James Sherk', 'David Muhlhausen']  2011-05-16                trade   \n",
       "4                     ['Kirk Johnson']  2005-04-06            education   \n",
       "\n",
       "                                   cleaned_text_sent  \n",
       "0  Yesterday the Senate decided to take up legisl...  \n",
       "1  The 2008 financial crisis was a major missed o...  \n",
       "2  It hasn't gotten much attention, but two big b...  \n",
       "3  The Obama Administration and Congress recently...  \n",
       "4  On April 5, 2005, the Georgetown Public Policy...  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data['date'] = cleaned_data['date'].apply(convert_date)\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('heritage_clean_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
